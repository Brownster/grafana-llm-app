# Grafana LLM App - Architecture Documentation

**Created:** 2025-12-12
**Purpose:** Technical reference for building the Grafana Copilot

---

## ğŸ“‹ Table of Contents

1. [System Overview](#system-overview)
2. [Frontend Architecture](#frontend-architecture)
3. [Backend Architecture](#backend-architecture)
4. [Data Flow](#data-flow)
5. [Key Components Reference](#key-components-reference)
6. [Integration Points](#integration-points)

---

## 1. System Overview

### Architecture Pattern
**3-Tier Architecture:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Frontend (React + TypeScript)                          â”‚
â”‚  - @grafana/llm library (npm package)                   â”‚
â”‚  - grafana-llm-app plugin UI                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚ HTTP/SSE + Grafana Live (WebSocket)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Backend (Go + Grafana Plugin SDK)                      â”‚
â”‚  - LLM Provider Proxy                                   â”‚
â”‚  - MCP Server (Model Context Protocol)                  â”‚
â”‚  - Streaming Handler                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚ HTTP/gRPC
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  External Services                                       â”‚
â”‚  - OpenAI / Azure / Anthropic APIs                      â”‚
â”‚  - Grafana APIs (dashboards, datasources, alerts)       â”‚
â”‚  - Vector Database (Qdrant / Grafana VectorAPI)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Technology Stack

**Frontend:**
- **Language:** TypeScript 5.6.2
- **Framework:** React 18.3.1
- **State Management:** React Hooks + RxJS 7.8.2
- **UI Library:** @grafana/ui components
- **Build Tool:** Rollup (library) + Webpack (plugin)

**Backend:**
- **Language:** Go 1.25.1
- **Framework:** Grafana Plugin SDK v0.281.0
- **MCP:** Model Context Protocol SDK (TypeScript + Go)
- **LLM SDKs:**
  - go-openai v1.41.2
  - anthropic-sdk-go v1.14.0

---

## 2. Frontend Architecture

### 2.1 Package Structure

```
packages/
â”œâ”€â”€ grafana-llm-frontend/          # @grafana/llm npm package
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ llm.ts                # Core LLM API functions
â”‚   â”‚   â”œâ”€â”€ mcp.tsx               # MCP client provider
â”‚   â”‚   â”œâ”€â”€ openai.ts             # OpenAI type definitions
â”‚   â”‚   â”œâ”€â”€ vector.ts             # Vector search utilities
â”‚   â”‚   â””â”€â”€ types.ts              # Shared TypeScript types
â”‚   â””â”€â”€ package.json
â”‚
â””â”€â”€ grafana-llm-app/               # Grafana app plugin
    â”œâ”€â”€ src/
    â”‚   â”œâ”€â”€ components/
    â”‚   â”‚   â””â”€â”€ AppConfig/
    â”‚   â”‚       â”œâ”€â”€ DevSandbox/   # ğŸ¯ CRITICAL: Chat UI to extract
    â”‚   â”‚       â”‚   â”œâ”€â”€ DevSandboxChat.tsx
    â”‚   â”‚       â”‚   â”œâ”€â”€ DevSandboxToolInspector.tsx
    â”‚   â”‚       â”‚   â””â”€â”€ types.ts
    â”‚   â”‚       â”œâ”€â”€ AppConfig.tsx
    â”‚   â”‚       â””â”€â”€ LLMConfig.tsx
    â”‚   â”œâ”€â”€ pages/
    â”‚   â”‚   â”œâ”€â”€ MainPage.tsx
    â”‚   â”‚   â”œâ”€â”€ MCPTools.tsx      # MCP tool inspector page
    â”‚   â”‚   â””â”€â”€ Models.tsx
    â”‚   â”œâ”€â”€ module.ts              # Plugin registration
    â”‚   â””â”€â”€ plugin.json            # Plugin metadata
    â””â”€â”€ package.json
```

### 2.2 Core Frontend Components

#### A. LLM API (`llm.ts`)

**Purpose:** Provides functions for interacting with LLM providers via the backend plugin.

**Key Functions:**

```typescript
// Check if LLM plugin is enabled
async function enabled(): Promise<boolean>

// Non-streaming chat completion
async function chatCompletions(request: ChatCompletionsRequest):
  Promise<ChatCompletionsResponse>

// Streaming chat completion (returns RxJS Observable)
function streamChatCompletions(request: ChatCompletionsRequest):
  Observable<ChatCompletionsResponse<ChatCompletionsChunk>>

// React hook for streaming with state management
function useLLMStream(options): {
  stream: (messages, tools) => void;
  reply: string;
  isLoading: boolean;
  error: Error | undefined;
}
```

**RxJS Operators:**
- `accumulateContent()` - Accumulates streaming content chunks
- `extractContent()` - Extracts content from response chunks
- `accumulateToolCalls()` - Accumulates tool call chunks

**API Endpoint:**
```
POST /api/plugins/grafana-llm-app/resources/llm/v1/chat/completions
```

**Request Format:** OpenAI-compatible chat completions API

---

#### B. MCP Client (`mcp.tsx`)

**Purpose:** Provides React components and utilities for MCP (Model Context Protocol) integration.

**Key Components:**

```typescript
// React context provider for MCP client
export const MCPClientProvider: React.FC<{
  transport?: Transport;
  children: React.ReactNode;
}>

// Hook to access MCP client
export const useMCPClient = (): {
  client: Client | null;
  isConnecting: boolean;
}

// Convert MCP tools to OpenAI function calling format
export function convertToolsToOpenAI(tools: MCPTool[]): OpenAITool[]
```

**Transport Options:**

1. **StreamableHTTPClientTransport (Recommended):**
   ```typescript
   const transport = new StreamableHTTPClientTransport(
     '/api/plugins/grafana-llm-app/resources/mcp/grafana',
     {
       fetch: (url, options) => {
         return getBackendSrv().fetch({ url, method: 'POST', data: options?.body }).toPromise();
       }
     }
   );
   ```

2. **GrafanaLiveTransport (Deprecated):**
   - Uses Grafana Live WebSocket
   - Being phased out in favor of HTTP transport

---

#### C. DevSandboxChat Component

**Location:** `packages/grafana-llm-app/src/components/AppConfig/DevSandbox/DevSandboxChat.tsx`

**â­ This is the component we'll extract in Phase 1**

**Key Features:**
- âœ… Message history management (`ChatMessage[]`)
- âœ… Streaming LLM responses with RxJS
- âœ… Tool call detection and execution
- âœ… Tool call visualization
- âœ… Auto-scrolling chat container
- âœ… Error handling

**Architecture Pattern:**

```typescript
// Message type
interface ChatMessage {
  role: 'user' | 'assistant';
  content: string;
  timestamp: Date;
}

// Component structure
export function DevSandboxChat({ useStream, toolCalls, setToolCalls }) {
  // 1. Get MCP client
  const { client } = mcp.useMCPClient();

  // 2. State management
  const [chatHistory, setChatHistory] = useState<ChatMessage[]>([]);
  const [currentInput, setCurrentInput] = useState('');
  const [isGenerating, setIsGenerating] = useState(false);

  // 3. Get available tools
  const { tools } = useAsync(() => client?.listTools());

  // 4. Streaming handler with tool calling loop
  const handleStreamingChatWithHistory = async (messages, tools) => {
    // Stream LLM response
    let stream = llm.streamChatCompletions({ model, messages, tools });

    // Partition into tool calls and content
    let [toolCallsStream, contentStream] = partition(stream, isToolCall);

    // Subscribe to content updates
    contentStream.pipe(llm.accumulateContent()).subscribe(updateChat);

    // Handle tool calls
    let toolCallMessages = await lastValueFrom(
      toolCallsStream.pipe(llm.accumulateToolCalls())
    );

    // Loop: Execute tools â†’ Get more LLM response â†’ Repeat until no tools
    while (toolCallMessages.tool_calls.length > 0) {
      await Promise.all(toolCallMessages.tool_calls.map(executeToolCall));
      // Continue with new stream...
    }
  };
}
```

**Tool Execution Pattern:**

```typescript
async function handleToolCall(fc, client, toolCalls, setToolCalls, messages) {
  // 1. Mark tool as running
  setToolCalls(new Map(toolCalls.set(id, { ...fc, running: true })));

  // 2. Execute via MCP client
  const response = await client.callTool({
    name: fc.function.name,
    arguments: JSON.parse(fc.function.arguments)
  });

  // 3. Add tool result to message history
  messages.push({
    role: 'tool',
    tool_call_id: id,
    content: extractTextContent(response)
  });

  // 4. Mark tool as completed
  setToolCalls(new Map(toolCalls.set(id, { ...fc, running: false, response })));
}
```

---

## 3. Backend Architecture

### 3.1 Package Structure

```
packages/grafana-llm-app/pkg/
â”œâ”€â”€ plugin/
â”‚   â”œâ”€â”€ plugin.go              # Main plugin entrypoint
â”‚   â”œâ”€â”€ routes.go              # HTTP route handlers
â”‚   â”œâ”€â”€ app.go                 # App instance setup
â”‚   â”œâ”€â”€ config/                # Configuration management
â”‚   â”œâ”€â”€ provider/              # LLM provider implementations
â”‚   â”‚   â”œâ”€â”€ openai_provider.go
â”‚   â”‚   â”œâ”€â”€ azure_provider.go
â”‚   â”‚   â”œâ”€â”€ anthropic_provider.go
â”‚   â”‚   â””â”€â”€ grafana_provider.go
â”‚   â””â”€â”€ vector/                # Vector search services
â”‚       â”œâ”€â”€ service.go
â”‚       â”œâ”€â”€ embedder.go
â”‚       â””â”€â”€ store.go
â”‚
â””â”€â”€ mcp/
    â”œâ”€â”€ mcp.go                 # MCP server setup
    â”œâ”€â”€ live_server.go         # Grafana Live MCP transport
    â”œâ”€â”€ auth.go                # Authentication handling
    â””â”€â”€ logger.go              # MCP logger implementation
```

### 3.2 Core Backend Components

#### A. Plugin Entrypoint (`plugin.go`)

**Purpose:** Grafana plugin SDK integration and lifecycle management.

**Key Structure:**

```go
type App struct {
    // Grafana plugin SDK backend instance
    backend.CallResourceHandler

    // LLM provider (OpenAI, Azure, Anthropic, etc.)
    llmProvider provider.LLMProvider

    // MCP (Model Context Protocol) server
    mcp *mcp.MCP

    // Configuration
    settings *config.AppSettings
}

// Grafana plugin SDK lifecycle methods
func (a *App) CheckHealth(ctx context.Context, req *backend.CheckHealthRequest)
    (*backend.CheckHealthResult, error)

func (a *App) CallResource(ctx context.Context, req *backend.CallResourceRequest,
    sender backend.CallResourceResponseSender) error
```

**HTTP Resource Routes:**
- `/llm/v1/chat/completions` - LLM chat completions (streaming + non-streaming)
- `/mcp/grafana` - MCP Streamable HTTP endpoint
- `/health` - Health check endpoint

---

#### B. MCP Server (`mcp.go`)

**Purpose:** Manages MCP server and tool registration.

**Key Structure:**

```go
// MCP represents the complete MCP infrastructure
type MCP struct {
    // Core MCP server (handles tool registration and execution)
    Server *server.MCPServer

    // Grafana Live server (WebSocket transport - deprecated)
    LiveServer *GrafanaLiveServer

    // HTTP server (modern transport)
    HTTPServer *server.StreamableHTTPServer

    // Configuration
    Settings Settings
}

// Initialize MCP with all Grafana tools
func New(settings Settings, pluginVersion string) (*MCP, error) {
    srv := server.NewMCPServer("grafana-llm-app", pluginVersion)

    // Register all tool categories from github.com/grafana/mcp-grafana
    tools.AddSearchTools(srv)        // Grafana resource search
    tools.AddDatasourceTools(srv)    // Query any datasource
    tools.AddPrometheusTools(srv)    // PromQL queries
    tools.AddLokiTools(srv)          // LogQL queries
    tools.AddDashboardTools(srv)     // Dashboard CRUD
    tools.AddAlertingTools(srv)      // Alert rules
    tools.AddIncidentTools(srv)      // Incident management
    tools.AddOnCallTools(srv)        // On-call schedules
    tools.AddAssertsTools(srv)       // Asserts integration
    tools.AddSiftTools(srv)          // Sift integration

    // Initialize transports (Live + HTTP)
    // ...

    return &MCP{Server: srv, ...}, nil
}
```

**MCP Tool Categories (from `github.com/grafana/mcp-grafana`):**

| Tool Category | Description | Example Tools |
|---------------|-------------|---------------|
| Search | Search Grafana resources | `grafana_search` |
| Datasource | Query any configured datasource | `datasource_query` |
| Prometheus | Execute PromQL queries | `prometheus_query_range`, `prometheus_query_instant` |
| Loki | Execute LogQL queries | `loki_query_range` |
| Dashboard | CRUD operations on dashboards | `grafana_get_dashboard`, `grafana_create_dashboard` |
| Alerting | Manage alert rules | `grafana_list_alert_rules`, `grafana_create_alert_rule` |
| Incident | Incident management | Incident CRUD operations |
| OnCall | On-call schedule access | Schedule and user settings |
| Asserts | Asserts integration | Asserts-specific tools |
| Sift | Sift integration | Sift-specific tools |

**Authentication:**
- **Service Account Token:** Default authentication (plugin token from Grafana)
- **On-Behalf-Of (Cloud):** Exchange access policy token for user-specific token

---

#### C. LLM Provider Abstraction

**Purpose:** Unified interface for multiple LLM providers.

**Pattern:**

```go
type LLMProvider interface {
    // Stream chat completions
    StreamChatCompletions(ctx context.Context, req ChatCompletionsRequest)
        (chan ChatCompletionsChunk, error)

    // Non-streaming chat completions
    ChatCompletions(ctx context.Context, req ChatCompletionsRequest)
        (ChatCompletionsResponse, error)

    // Health check
    Enabled(ctx context.Context) (HealthCheckResponse, error)
}

// Implementations:
// - OpenAIProvider (go-openai)
// - AzureProvider (Azure OpenAI)
// - AnthropicProvider (anthropic-sdk-go)
// - GrafanaProvider (llm-gateway)
```

**Model Tiers:**
- `base` - Efficient, high-throughput (e.g., GPT-4o Mini, Claude Sonnet)
- `large` - Advanced with longer context (e.g., GPT-4o, Claude Sonnet)

---

## 4. Data Flow

### 4.1 Chat Message Flow (No Tool Calls)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User Input   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Frontend (DevSandboxChat.tsx)               â”‚
â”‚ - Add user message to chatHistory           â”‚
â”‚ - Call llm.streamChatCompletions()          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ HTTP POST + SSE
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Backend (plugin/routes.go)                  â”‚
â”‚ - Route: /llm/v1/chat/completions           â”‚
â”‚ - Forward to LLM Provider                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ HTTP/gRPC
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LLM Provider (OpenAI/Azure/Anthropic)       â”‚
â”‚ - Generate response tokens                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ Streaming response
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Backend                                      â”‚
â”‚ - Proxy chunks to frontend via SSE          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ RxJS Observable
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Frontend                                     â”‚
â”‚ - accumulateContent() operator              â”‚
â”‚ - Update chatHistory with streaming content â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.2 Chat Message Flow (With Tool Calls)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User Input   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Frontend: Send to LLM with tools            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LLM: Generates tool_calls                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Frontend: Detect tool_calls                 â”‚
â”‚ - partition() stream into tool calls vs content â”‚
â”‚ - accumulateToolCalls() operator            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Frontend: Execute tool via MCP client       â”‚
â”‚ - client.callTool({ name, arguments })      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ HTTP POST
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Backend: MCP HTTP Server                    â”‚
â”‚ - Route: /mcp/grafana                       â”‚
â”‚ - Dispatch to registered tool handler       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Tool Handler (mcp-grafana package)          â”‚
â”‚ - Execute Grafana API call                  â”‚
â”‚ - Query datasource / Fetch dashboard / etc. â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Frontend: Receive tool result               â”‚
â”‚ - Add to messages as role: 'tool'           â”‚
â”‚ - Send back to LLM with updated history     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LLM: Interprets tool result                 â”‚
â”‚ - Generates natural language response       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Frontend: Display final response            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Loop Continues:** If LLM generates more tool_calls, repeat the tool execution cycle.

---

## 5. Key Components Reference

### 5.1 Frontend Components to Extract (Phase 1)

#### Components to Copy/Refactor:

| Component | Source | Destination | Purpose |
|-----------|--------|-------------|---------|
| `DevSandboxChat` | `DevSandbox/DevSandboxChat.tsx` | `Copilot/CopilotChat.tsx` | Main chat interface |
| Message rendering | Inline in DevSandboxChat | `Copilot/CopilotMessageList.tsx` | Message display with markdown |
| Input box | Inline in DevSandboxChat | `Copilot/CopilotInput.tsx` | Message input with suggested prompts |
| Tool call view | `DevSandbox/DevSandboxToolInspector.tsx` | `Copilot/CopilotToolCallView.tsx` | Tool execution visualization |

#### Hooks to Create:

| Hook | File | Purpose |
|------|------|---------|
| `useCopilotChat` | `Copilot/hooks/useCopilotChat.ts` | Chat state + streaming logic |
| `useGrafanaContext` | `Copilot/hooks/useGrafanaContext.ts` | Extract dashboard context |
| `useToolExecution` | `Copilot/hooks/useToolExecution.ts` | MCP tool execution |

#### Utilities to Create:

| Utility | File | Purpose |
|---------|------|---------|
| `buildSystemPrompt` | `Copilot/utils/systemPrompts.ts` | Generate context-aware prompts |
| `messageFormatting` | `Copilot/utils/messageFormatting.ts` | Markdown rendering, syntax highlighting |
| `conversationStorage` | `Copilot/utils/conversationStorage.ts` | localStorage persistence |

---

### 5.2 Backend Components to Extend (Phase 4)

#### For Knowledge Base Integration:

| Component | File | Purpose |
|-----------|------|---------|
| `KnowledgeBaseService` | `pkg/plugin/vector/knowledge.go` (NEW) | Vector search for docs/runbooks |
| `AddKnowledgeBaseTools` | `pkg/mcp/knowledge_tools.go` (NEW) | MCP tools for search/ingest |
| Knowledge base ingestion | `scripts/ingest-knowledge.ts` (NEW) | CLI tool for bulk ingestion |

---

## 6. Integration Points

### 6.1 Grafana Services Used

**Frontend (`@grafana/runtime`):**
```typescript
import {
  getBackendSrv,      // HTTP requests to backend
  getTemplateSrv,     // Template variables
  locationService,    // URL/routing
  getGrafanaLiveSrv   // WebSocket (deprecated for MCP)
} from '@grafana/runtime';
```

**Backend (Grafana Plugin SDK):**
```go
import (
    "github.com/grafana/grafana-plugin-sdk-go/backend"
    "github.com/grafana/grafana-plugin-sdk-go/backend/log"
    "github.com/grafana/grafana-plugin-sdk-go/backend/resource/httpadapter"
)
```

### 6.2 External Dependencies

**LLM Providers:**
- OpenAI API (go-openai)
- Azure OpenAI API
- Anthropic API (anthropic-sdk-go)
- Grafana LLM Gateway (Cloud)

**Vector Services:**
- Qdrant (vector database)
- Grafana VectorAPI (Cloud)
- OpenAI Embeddings API

**MCP:**
- `@modelcontextprotocol/sdk` (TypeScript)
- `github.com/mark3labs/mcp-go` (Go)
- `github.com/grafana/mcp-grafana` (Tool implementations)

---

## 7. Security & Authentication

### 7.1 API Key Storage

**Storage:** Encrypted in Grafana's plugin settings database

**Access:** Backend reads via `ctx.DataSourceInstanceSettings`

**User Never Sees:** API keys are write-only from UI, never read back

### 7.2 IAM Permissions (from plugin.json)

The plugin has extensive permissions granted:

```json
{
  "iam": {
    "permissions": [
      { "action": "datasources:read" },
      { "action": "datasources:query" },
      { "action": "dashboards:read" },
      { "action": "dashboards:create" },
      { "action": "dashboards:write" },
      { "action": "folders:read" },
      { "action": "alert.rules:read" },
      // ... OnCall, IRM, Incident app access
    ]
  }
}
```

**Implication:** The copilot can:
- âœ… Read all dashboards and panels
- âœ… Create and modify dashboards
- âœ… Query all datasources
- âœ… Read alert rules
- âœ… Access incident/on-call data

**Does NOT have:**
- âŒ User/team write permissions
- âŒ Datasource configuration write
- âŒ Admin API access

---

## 8. Development Workflow

### 8.1 Starting Dev Environment

```bash
# Terminal 1: Start dev builds (watches for changes)
./dev.sh

# Terminal 2: Start Grafana server
npm run server

# Access: http://localhost:3000
```

### 8.2 Making Changes

**Frontend changes:**
- Edit files in `packages/grafana-llm-frontend/src/` or `packages/grafana-llm-app/src/`
- Webpack/Rollup automatically rebuilds
- Refresh browser to see changes

**Backend changes:**
- Edit files in `packages/grafana-llm-app/pkg/`
- Run: `npm run backend:restart`
- Plugin automatically reloads in Grafana

### 8.3 Testing

```bash
# Run all tests
npm run test:ci

# Run E2E tests
npm run test:e2e

# Run backend tests only
npm run backend:test
```

---

## 9. Next Steps (Implementation Phases)

### Phase 1: Extract Chat UI
1. Copy `DevSandboxChat.tsx` â†’ `CopilotChat.tsx`
2. Extract hooks: `useCopilotChat`, `useToolExecution`
3. Create `CopilotDrawer` component (Grafana `<Drawer>` component)
4. Register in `module.ts` as global component
5. Add conversation persistence (localStorage)

### Phase 2: Context Awareness
1. Create `useGrafanaContext` hook
2. Extract dashboard UID, title, panels, variables, time range
3. Create `buildSystemPrompt` utility
4. Inject context into system message

### Phase 3: Enable MCP Tools
1. Verify all MCP tools in `mcp.go`
2. Test tool execution from frontend
3. Add tool call visualization in UI
4. Implement error handling for failed tools

### Phase 4: Knowledge Base
1. Extend vector service: `KnowledgeBaseService`
2. Create MCP tools: `search_knowledge_base`, `ingest_knowledge_document`
3. Build ingestion pipeline (CLI script)
4. Update system prompt for RAG usage
5. Test with sample documents

### Phase 5-7: Polish, Test, Deploy
- See roadmap.md for detailed steps

---

## 10. Glossary

**MCP (Model Context Protocol):** Standard protocol for connecting LLMs to tools/data sources

**SSE (Server-Sent Events):** HTTP-based streaming protocol used for LLM responses

**RxJS:** Reactive programming library for handling async data streams

**RAG (Retrieval Augmented Generation):** Pattern where LLM queries knowledge base before generating response

**Grafana Live:** Grafana's WebSocket-based real-time messaging system

**Tool Calling:** LLM feature where model can request execution of predefined functions

**Streaming:** Real-time token-by-token response delivery (vs waiting for complete response)

---

## 11. Troubleshooting

### Common Issues

**"LLM plugin not configured":**
- Navigate to Configuration â†’ Plugins â†’ Grafana LLM App
- Configure an LLM provider (OpenAI, Azure, or Anthropic)
- Add API key and test connection

**"MCP client not initialized":**
- Ensure `MCPClientProvider` wraps your component
- Check browser console for connection errors
- Verify `/mcp/grafana` endpoint is accessible

**Tool execution fails:**
- Check IAM permissions in plugin.json
- Verify service account has required permissions
- Check backend logs for detailed error

**Backend build fails with Go version error:**
- Use `./dev.sh` script (sets GOTOOLCHAIN=auto)
- Or manually set: `export GOTOOLCHAIN=auto`

---

**End of Architecture Documentation**
